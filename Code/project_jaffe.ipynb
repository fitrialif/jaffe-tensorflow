{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv # opencv\n",
    "import tensorflow as tf #tensorflow\n",
    "import glob # to read files\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirImage = 'jaffe/'\n",
    "filenames = []\n",
    "filenames += glob.glob(dirImage+\"/*\"+\".tiff\")\n",
    "# print(\"Processing \" + str(len(filenames)) + \" images\")\n",
    "# print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for file in filenames:\n",
    "    img = np.asarray(cv.imread(file, 0))\n",
    "    images.append(img)\n",
    "images = np.asarray(images)\n",
    "# print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_count = 7\n",
    "TRAINING_SIZE = 170\n",
    "VALIDATION_SIZE = len(images) - TRAINING_SIZE\n",
    "train_images = images[:TRAINING_SIZE, :, :] # 170, 256, 256\n",
    "test_images = images[TRAINING_SIZE:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_file = open('temp_labels.txt');\n",
    "label_data = label_file.read()\n",
    "labels = []\n",
    "for i in xrange(len(label_data)):\n",
    "    if (label_data[i]!='\\n'):\n",
    "        labels.append(float(label_data[i]))\n",
    "train_labels = np.asarray(labels[:TRAINING_SIZE], dtype=int)\n",
    "test_labels = np.asarray(labels[170:], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = len(labels_dense)\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    # print(type(np.int_(index_offset + labels_dense.ravel())))\n",
    "    labels_one_hot.flat[np.int_(index_offset + labels_dense.ravel())] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = dense_to_one_hot(train_labels.ravel(), labels_count)\n",
    "# labels = labels.astype(np.uint8)\n",
    "test_labels = dense_to_one_hot(test_labels.ravel(), labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(TRAINING_SIZE, -1)\n",
    "test_images = test_images.reshape(VALIDATION_SIZE, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat pixel values is 65536\n"
     ]
    }
   ],
   "source": [
    "train_image_pixels = train_images.shape[1]\n",
    "print 'Flat pixel values is %d'%(train_image_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_width = np.int_(np.ceil(np.sqrt(train_image_pixels)))\n",
    "train_image_height = np.int_(np.ceil(np.sqrt(train_image_pixels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(train_image_width)\n",
    "print(train_image_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TF CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
    "    # setup input filter state\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "    \n",
    "    # initialise weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup convolutional layer \n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "    out_layer += bias\n",
    "    \n",
    "    # applying relu non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    \n",
    "    # performing max pooling\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, padding='SAME')\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input & output of NN\n",
    "\n",
    "# images\n",
    "x = tf.placeholder(tf.float32, [None, train_image_width*train_image_height])\n",
    "# dynamically reshaping input\n",
    "x_shaped = tf.reshape(x, [-1, train_image_width, train_image_height, 1])\n",
    "# labels\n",
    "y = tf.placeholder(tf.float32, [None, labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating sparse layers of CNN\n",
    "layer1 = new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
    "layer2 = new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n",
    "layer3 = new_conv_layer(layer2, 64, 128, [5, 5], [2, 2], name='layer3')\n",
    "layer4 = new_conv_layer(layer3, 128, 256, [5, 5], [2, 2], name='layer4')\n",
    "flattened = tf.reshape(layer4, [-1, 16 * 16 * 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculating dense layers of CNN\n",
    "wd1 = tf.Variable(tf.truncated_normal([16 * 16 * 256, 1000], stddev=0.03), name='wd1')\n",
    "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "# dropout for reducing overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(dense_layer1, keep_prob)\n",
    "\n",
    "# another layer for softmax calculation and readout\n",
    "wd2 = tf.Variable(tf.truncated_normal([1000, labels_count], stddev=0.03), name='wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([labels_count], stddev=0.01), name='bd2')\n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "y_ = tf.nn.softmax(dense_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross entropy cost function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to 3000 iterations \n",
    "epochs = 10\n",
    "DROPOUT = 0.5\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, 'cost =', '293.017', 'train accuracy: 0.318', 'test accuracy: 0.209')\n",
      "('Epoch:', 2, 'cost =', '133.675', 'train accuracy: 0.335', 'test accuracy: 0.302')\n",
      "('Epoch:', 3, 'cost =', '143.813', 'train accuracy: 0.229', 'test accuracy: 0.186')\n",
      "('Epoch:', 4, 'cost =', '164.441', 'train accuracy: 0.459', 'test accuracy: 0.372')\n",
      "('Epoch:', 5, 'cost =', '120.500', 'train accuracy: 0.382', 'test accuracy: 0.302')\n",
      "('Epoch:', 6, 'cost =', '81.784', 'train accuracy: 0.406', 'test accuracy: 0.326')\n",
      "('Epoch:', 7, 'cost =', '95.721', 'train accuracy: 0.429', 'test accuracy: 0.349')\n",
      "('Epoch:', 8, 'cost =', '86.154', 'train accuracy: 0.388', 'test accuracy: 0.349')\n",
      "('Epoch:', 9, 'cost =', '92.828', 'train accuracy: 0.482', 'test accuracy: 0.349')\n",
      "('Epoch:', 10, 'cost =', '100.317', 'train accuracy: 0.465', 'test accuracy: 0.326')\n",
      "\n",
      "Training complete!\n",
      "0.325581\n"
     ]
    }
   ],
   "source": [
    "# adding optimiser\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# defining accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# setting up the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# recording variable to store accuracy\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('logs')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialising variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(train_labels) / batch_size)\n",
    "    for epoch in xrange(epochs):\n",
    "        avg_cost = 0\n",
    "        batch_index = 0\n",
    "        for j in xrange(total_batch):\n",
    "            train_batch_images = train_images[batch_index:(batch_index+batch_size), :]\n",
    "            train_batch_labels = train_labels[batch_index:(batch_index+batch_size), :]\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x:train_batch_images, y:train_batch_labels, keep_prob:0.5})\n",
    "            avg_cost += c\n",
    "            batch_index += batch_size\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:train_images, y:train_labels})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: test_images, y: test_labels})\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \"train accuracy: {:.3f}\".format(train_acc), \"test accuracy: {:.3f}\".format(test_acc))\n",
    "        summary = sess.run(merged, feed_dict={x: test_images, y: test_labels})\n",
    "        writer.add_summary(summary, epoch)\n",
    "    print(\"\\nTraining complete!\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    print(sess.run(accuracy, feed_dict={x: test_images, y: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
